{"cells":[{"source":"![dvd_image](dvd_image.jpg)\n\nA DVD rental company needs your help! They want to figure out how many days a customer will rent a DVD for based on some features and has approached you for help. They want you to try out some regression models which will help predict the number of days a customer will rent a DVD for. The company wants a model which yeilds a MSE of 3 or less on a test set. The model you make will help the company become more efficient inventory planning.\n\nThe data they provided is in the csv file `rental_info.csv`. It has the following features:\n- `\"rental_date\"`: The date (and time) the customer rents the DVD.\n- `\"return_date\"`: The date (and time) the customer returns the DVD.\n- `\"amount\"`: The amount paid by the customer for renting the DVD.\n- `\"amount_2\"`: The square of `\"amount\"`.\n- `\"rental_rate\"`: The rate at which the DVD is rented for.\n- `\"rental_rate_2\"`: The square of `\"rental_rate\"`.\n- `\"release_year\"`: The year the movie being rented was released.\n- `\"length\"`: Length of the movie being rented, in minutes.\n- `\"length_2\"`: The square of `\"length\"`.\n- `\"replacement_cost\"`: The amount it will cost the company to replace the DVD.\n- `\"special_features\"`: Any special features, for example trailers/deleted scenes that the DVD also has.\n- `\"NC-17\"`, `\"PG\"`, `\"PG-13\"`, `\"R\"`: These columns are dummy variables of the rating of the movie. It takes the value 1 if the move is rated as the column name and 0 otherwise. For your convinience, the reference dummy has already been dropped.","metadata":{},"id":"b4ae5707-109f-4cd6-8168-88cac0179d6b","cell_type":"markdown"},{"source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.linear_model import Lasso, LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor\nimport matplotlib.pyplot as plt\n\nSEED = 9\n\n# 1. Preprocessing - Getting the # of rental days\ndf = pd.read_csv('rental_info.csv')\ndf['rental_date'] = pd.to_datetime(df['rental_date'])\ndf['return_date'] = pd.to_datetime(df['return_date'])\ndf['rental_length_days'] = (df['return_date'] - df['rental_date']).dt.days\n\n# 2. Preprocessing - Adding 2 columns of dummy variables using the special column\ndf['special_features'] = df['special_features'].astype('str')\ndf['deleted_scenes'] = np.where(df['special_features'].str.contains('Deleted Scenes'), 1, 0)\ndf['behind_the_scenes'] = np.where(df['special_features'].str.contains('Behind the Scenes'), 1, 0)\n\n# 3. Assigning the training & testing datasets\nsubset = df.drop(columns=['rental_date', 'return_date', 'special_features'])\nX = subset.drop(columns='rental_length_days')\ny = subset['rental_length_days']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n\n# 4. Performing feature selection by Lasso Regression\nlasso = Lasso(alpha=0.01, random_state=SEED, tol=0.0001, max_iter=1000)\nlasso.fit(X_train, y_train)\nlasso_coef = lasso.coef_\n\n# Select features with non-zero coefficients\nX_train_lasso, X_test_lasso = X_train.iloc[:, lasso_coef > 0], X_test.iloc[:, lasso_coef > 0]\n\n# 5. Choosing models and performing hyperparameter tuning\nscaler = StandardScaler()\nlr = LinearRegression()\ndt = DecisionTreeRegressor(random_state=SEED)\nrf = RandomForestRegressor(random_state=SEED)\nsgbr = GradientBoostingRegressor(random_state=SEED)\nbr = BaggingRegressor(random_state=SEED)\n\nmodels = [lr, dt, rf, sgbr, br, lasso]\nmse = []\n\nparams_dt = {\n    'max_depth': range(1, 50),\n    'min_samples_leaf': np.linspace(0.01, 0.5, 50),\n    'max_features': np.linspace(0.1, 1.0, 20)\n}\n\nparams_rf = {\n    'n_estimators': range(100, 500, 100),\n    'max_depth': range(1, 20),\n    'min_samples_leaf': np.linspace(0.01, 0.5, 50),\n    'max_features': ['auto', 'sqrt', 'log2']\n}\n\nparams_sgbr = {\n    'n_estimators': range(100, 500, 100),\n    'max_depth': range(1, 15),\n    'min_samples_leaf': np.linspace(0.01, 0.5, 50),\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'subsample': np.linspace(0.5, 1.0, 20)\n}\n\nparams_br = {\n    'n_estimators': range(100, 500, 100),\n    'max_features': np.linspace(0.1, 1.0, 10),\n    'max_samples': np.linspace(0.5, 1.0, 10),\n    'bootstrap': [True, False],\n    'bootstrap_features': [True, False],\n    'oob_score': [True, False]\n}\n\nparams_lasso = {\n    'alpha': np.logspace(-4, 0, 50),\n    'max_iter': [1000, 5000, 10000],\n    'tol': [1e-4, 1e-3, 1e-2]\n}\n\n# Mapping models to parameter grids\nmodel_params = {\n    lr: None,\n    dt: params_dt,\n    rf: params_rf,\n    sgbr: params_sgbr,\n    br: params_br,\n    lasso: params_lasso\n}\n\n# Evaluate each model\nfor model, grid in model_params.items():\n    if grid:\n        cv = RandomizedSearchCV(model, grid, random_state=SEED, cv=10, n_iter=10, n_jobs=-1)\n        cv.fit(scaler.fit_transform(X_train_lasso), y_train)\n        print(f'Best params for {model.__class__.__name__}: {cv.best_params_}')\n        model = cv.best_estimator_\n    else:\n        model.fit(scaler.fit_transform(X_train_lasso), y_train)\n\n    y_pred = model.predict(scaler.transform(X_test_lasso))\n    mse.append(MSE(y_test, y_pred))\n\nprint(\"MSE for each model:\", mse)\n\nbest_model = model; best_mse = np.min(mse)","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"lastExecutedByKernel":null,"outputsMetadata":{"0":{"height":249,"type":"stream"}}},"id":"a7ede566-910a-445c-b11a-68d192ac8506","cell_type":"code","execution_count":112,"outputs":[{"output_type":"stream","name":"stdout","text":"Best params for DecisionTreeRegressor: {'min_samples_leaf': 0.01, 'max_features': 0.38421052631578945, 'max_depth': 38}\nBest params for RandomForestRegressor: {'n_estimators': 100, 'min_samples_leaf': 0.01, 'max_features': 'sqrt', 'max_depth': 11}\nBest params for GradientBoostingRegressor: {'subsample': 0.9210526315789473, 'n_estimators': 300, 'min_samples_leaf': 0.15000000000000002, 'max_features': 'auto', 'max_depth': 6}\nBest params for BaggingRegressor: {'oob_score': True, 'n_estimators': 400, 'max_samples': 0.7222222222222222, 'max_features': 1.0, 'bootstrap_features': False, 'bootstrap': True}\nBest params for Lasso: {'tol': 0.01, 'max_iter': 1000, 'alpha': 0.0062505519252739694}\nMSE for each model: [4.846638768993797, 4.205060840610949, 4.227812332758292, 3.851487812133041, 2.407725382093919, 4.846235169618719]\n"}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}